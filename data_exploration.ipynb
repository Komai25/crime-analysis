{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "latex"
        }
      },
      "source": [
        "# Crime Data - Exploratory Data Analysis\n",
        "\n",
        "This notebook provides an exploratory analysis of the Los Angeles crime dataset to understand patterns, relationships, and insights that will inform our machine learning approach.\n",
        "\n",
        "## Analysis Sections:\n",
        "1. **Data Loading & Overview**\n",
        "2. **Temporal Analysis** - Crime patterns over time\n",
        "3. **Spatial Analysis** - Geographic crime distribution\n",
        "4. **Crime Type Analysis** - Understanding different crime categories\n",
        "5. **Victim Demographics** - Age, gender, descent patterns\n",
        "6. **Location & Context** - Premises and environmental factors\n",
        "7. **Correlation Analysis** - Feature relationships\n",
        "8. **Missing Data Patterns** - Data quality assessment\n",
        "9. **Key Insights & Recommendations**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from scipy import stats\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "\n",
        "plt.style.use('default')\n",
        "sns.set_palette(\"husl\")\n",
        "plt.rcParams['figure.figsize'] = (12, 8)\n",
        "plt.rcParams['font.size'] = 10\n",
        "%matplotlib inline\n",
        "\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "pd.set_option('display.max_colwidth', 50)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Loading Crime Dataset...\")\n",
        "\n",
        "\n",
        "df = pd.read_csv('./data/crime_data.csv')\n",
        "\n",
        "\n",
        "# Basic dataset information\n",
        "print(f\"\\nDataset Overview:\")\n",
        "print(f\"   Shape: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
        "print(f\"   Memory: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "print(f\"   Date Range: {df['DATE OCC'].min()} to {df['DATE OCC'].max()}\")\n",
        "\n",
        "# Display first few rows\n",
        "print(f\"\\nFirst 3 rows:\")\n",
        "df.head(3)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### 2.1 Comprehensive Data Information Analysis\n",
        "\n",
        "This analysis provides detailed information about each column in the dataset, including:\n",
        "- Data types for each feature\n",
        "- Missing value counts and percentages\n",
        "- Number of unique values per feature\n",
        "- Overall data quality assessment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive data information\n",
        "print(\"COMPREHENSIVE DATA INFORMATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Column information\n",
        "print(\"\\nColumn Information:\")\n",
        "print(\"-\" * 30)\n",
        "for i, col in enumerate(df.columns, 1):\n",
        "    dtype = df[col].dtype\n",
        "    null_count = df[col].isnull().sum()\n",
        "    null_pct = (null_count / len(df)) * 100\n",
        "    unique_count = df[col].nunique()\n",
        "    \n",
        "    print(f\"{i:2d}. {col:<20} | {str(dtype):<10} | {null_count:>6} nulls ({null_pct:>5.1f}%) | {unique_count:>6} unique\")\n",
        "\n",
        "# Data types summary\n",
        "print(f\"\\nData Types Summary:\")\n",
        "print(\"-\" * 25)\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "# Missing values summary\n",
        "print(f\"\\nMissing Values Summary:\")\n",
        "print(\"-\" * 28)\n",
        "missing_summary = df.isnull().sum().sort_values(ascending=False)\n",
        "missing_summary = missing_summary[missing_summary > 0]\n",
        "if len(missing_summary) > 0:\n",
        "    for col, count in missing_summary.items():\n",
        "        pct = (count / len(df)) * 100\n",
        "        print(f\"{col:<20}: {count:>6} ({pct:>5.1f}%)\")\n",
        "else:\n",
        "    print(\"No missing values found!\")\n",
        "\n",
        "print(f\"\\nData Information Analysis Complete!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 3. Temporal Analysis - Crime Patterns Over Time\n",
        "\n",
        "Understanding temporal patterns is crucial for crime prediction and resource allocation. This section explores:\n",
        "- **Daily patterns** - Hour of day when crimes occur\n",
        "- **Weekly patterns** - Day of week variations\n",
        "- **Monthly patterns** - Seasonal trends\n",
        "- **Yearly trends** - Long-term changes\n",
        "- **Time-based correlations** - Relationships between time and crime types\n",
        "\n",
        "### 3.1 Feature Engineering for Temporal Analysis\n",
        "\n",
        "We create time-based features from the raw date and time columns to facilitate temporal analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporal feature engineering\n",
        "print(\"Temporal Feature Engineering\")\n",
        "\n",
        "# Convert date columns to datetime\n",
        "df['DATE OCC'] = pd.to_datetime(df['DATE OCC'])\n",
        "df['Date Rptd'] = pd.to_datetime(df['Date Rptd'])\n",
        "\n",
        "# Extract temporal features\n",
        "df['year'] = df['DATE OCC'].dt.year\n",
        "df['month'] = df['DATE OCC'].dt.month\n",
        "df['day'] = df['DATE OCC'].dt.day\n",
        "df['day_of_week'] = df['DATE OCC'].dt.dayofweek\n",
        "df['day_name'] = df['DATE OCC'].dt.day_name()\n",
        "df['month_name'] = df['DATE OCC'].dt.month_name()\n",
        "\n",
        "# Convert TIME OCC to hour\n",
        "df['hour'] = df['TIME OCC'] // 100\n",
        "df['minute'] = df['TIME OCC'] % 100\n",
        "\n",
        "# Create time categories\n",
        "def categorize_time(hour):\n",
        "    if 6 <= hour < 12:\n",
        "        return 'Morning'\n",
        "    elif 12 <= hour < 18:\n",
        "        return 'Afternoon'\n",
        "    elif 18 <= hour < 24:\n",
        "        return 'Evening'\n",
        "    else:\n",
        "        return 'Night'\n",
        "\n",
        "df['time_category'] = df['hour'].apply(categorize_time)\n",
        "\n",
        "# Create season from month\n",
        "def get_season(month):\n",
        "    if month in [12, 1, 2]:\n",
        "        return 'Winter'\n",
        "    elif month in [3, 4, 5]:\n",
        "        return 'Spring'\n",
        "    elif month in [6, 7, 8]:\n",
        "        return 'Summer'\n",
        "    else:\n",
        "        return 'Automn'\n",
        "\n",
        "df['season'] = df['month'].apply(get_season)\n",
        "\n",
        "print(f\"Temporal features created:\")\n",
        "print(f\"    Date range: {df['DATE OCC'].min().strftime('%Y-%m-%d')} to {df['DATE OCC'].max().strftime('%Y-%m-%d')}\")\n",
        "print(f\"    Time range: {df['hour'].min()}:00 to {df['hour'].max()}:00\")\n",
        "print(f\"    Years covered: {df['year'].nunique()} years ({df['year'].min()}-{df['year'].max()})\")\n",
        "print(f\"    Total days: {(df['DATE OCC'].max() - df['DATE OCC'].min()).days} days\")\n",
        "\n",
        "# Display sample of temporal features\n",
        "print(f\"\\nSample of temporal features:\")\n",
        "temporal_cols = ['DATE OCC', 'TIME OCC', 'year', 'month', 'day', 'hour', 'day_name', 'time_category', 'season']\n",
        "df[temporal_cols].head(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Temporal Patterns Visualization\n",
        "print(\"\\nTEMPORAL PATTERNS VISUALIZATION\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Create a comprehensive temporal analysis plot\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "fig.suptitle('Crime Temporal Patterns Analysis', fontsize=16, y=1.02)\n",
        "\n",
        "# 1. Hourly distribution\n",
        "hourly_crimes = df['hour'].value_counts().sort_index()\n",
        "axes[0, 0].bar(hourly_crimes.index, hourly_crimes.values, color='skyblue', alpha=0.8)\n",
        "axes[0, 0].set_title('Crimes by Hour of Day')\n",
        "axes[0, 0].set_xlabel('Hour')\n",
        "axes[0, 0].set_ylabel('Number of Crimes')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Day of week distribution\n",
        "day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
        "day_crimes = df['day_name'].value_counts().reindex(day_order)\n",
        "axes[0, 1].bar(day_crimes.index, day_crimes.values, color='lightcoral', alpha=0.8)\n",
        "axes[0, 1].set_title('Crimes by Day of Week')\n",
        "axes[0, 1].set_xlabel('Day of Week')\n",
        "axes[0, 1].set_ylabel('Number of Crimes')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Monthly distribution\n",
        "month_order = ['January', 'February', 'March', 'April', 'May', 'June',\n",
        "               'July', 'August', 'September', 'October', 'November', 'December']\n",
        "month_crimes = df['month_name'].value_counts().reindex(month_order)\n",
        "axes[0, 2].bar(month_crimes.index, month_crimes.values, color='lightgreen', alpha=0.8)\n",
        "axes[0, 2].set_title('Crimes by Month')\n",
        "axes[0, 2].set_xlabel('Month')\n",
        "axes[0, 2].set_ylabel('Number of Crimes')\n",
        "axes[0, 2].tick_params(axis='x', rotation=45)\n",
        "axes[0, 2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Time category distribution\n",
        "time_crimes = df['time_category'].value_counts()\n",
        "axes[1, 0].pie(time_crimes.values, labels=time_crimes.index, autopct='%1.1f%%', startangle=90)\n",
        "axes[1, 0].set_title('Crime Distribution by Time Category')\n",
        "\n",
        "# 5. Seasonal distribution\n",
        "season_crimes = df['season'].value_counts()\n",
        "axes[1, 1].pie(season_crimes.values, labels=season_crimes.index, autopct='%1.1f%%', startangle=90)\n",
        "axes[1, 1].set_title('Crime Distribution by Season')\n",
        "\n",
        "# 6. Yearly trend\n",
        "yearly_crimes = df['year'].value_counts().sort_index()\n",
        "axes[1, 2].plot(yearly_crimes.index, yearly_crimes.values, marker='o', linewidth=2, markersize=8)\n",
        "axes[1, 2].set_title('Crime Trend Over Years')\n",
        "axes[1, 2].set_xlabel('Year')\n",
        "axes[1, 2].set_ylabel('Number of Crimes')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print key insights\n",
        "print(\"\\nKEY TEMPORAL INSIGHTS:\")\n",
        "print(\"-\" * 30)\n",
        "peak_hour = hourly_crimes.idxmax()\n",
        "peak_day = day_crimes.idxmax()\n",
        "peak_month = month_crimes.idxmax()\n",
        "peak_season = season_crimes.idxmax()\n",
        "peak_time_category = time_crimes.idxmax()\n",
        "\n",
        "print(f\"Top crime hour: {peak_hour}:00 ({hourly_crimes[peak_hour]:,} crimes)\")\n",
        "print(f\"Top crime day: {peak_day} ({day_crimes[peak_day]:,} crimes)\")\n",
        "print(f\"Top crime month: {peak_month} ({month_crimes[peak_month]:,} crimes)\")\n",
        "print(f\"Top crime season: {peak_season} ({season_crimes[peak_season]:,} crimes)\")\n",
        "print(f\"Top time category: {peak_time_category} ({time_crimes[peak_time_category]:,} crimes)\")\n",
        "\n",
        "# Crime intensity analysis\n",
        "print(f\"\\n Crime Intensity Analysis:\")\n",
        "print(f\"    Average crimes per hour: {len(df) / 24:.1f}\")\n",
        "print(f\"    Average crimes per day: {len(df) / 7:.1f}\")\n",
        "print(f\"    Average crimes per month: {len(df) / 12:.1f}\")\n",
        "print(f\"    Average crimes per season: {len(df) / 4:.1f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Spatial Analysis - Geographic Crime Distribution\n",
        "\n",
        "Understanding the spatial distribution of crimes helps identify:\n",
        "- **Hot spots** - Areas with high crime concentration\n",
        "- **Safe zones** - Areas with low crime rates\n",
        "- **Geographical patterns** - Urban vs suburban crime patterns\n",
        "- **Area-specific trends** - Different crime types in different areas\n",
        "- **Coordinate analysis** - Latitude/longitude insights\n",
        "\n",
        "### 4.1 Geographic Distribution Analysis\n",
        "\n",
        "This analysis examines crime distribution across different areas of Los Angeles and provides spatial insights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Spatial Analysis - Geographic Crime Distribution\n",
        "print(\"SPATIAL ANALYSIS - GEOGRAPHIC PATTERNS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Basic spatial statistics\n",
        "print(\"Geographic Coverage:\")\n",
        "print(f\"   Latitude range: {df['LAT'].min():.4f} to {df['LAT'].max():.4f}\")\n",
        "print(f\"   Longitude range: {df['LON'].min():.4f} to {df['LON'].max():.4f}\")\n",
        "print(f\"   Unique coordinates: {df[['LAT', 'LON']].drop_duplicates().shape[0]:,}\")\n",
        "\n",
        "# Area analysis\n",
        "print(f\"\\nArea Analysis:\")\n",
        "area_crimes = df['AREA NAME'].value_counts().head(10)\n",
        "print(\"Top 10 Crime Areas:\")\n",
        "for i, (area, count) in enumerate(area_crimes.items(), 1):\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"{i:2d}. {area:<25}: {count:>6,} crimes ({percentage:>5.1f}%)\")\n",
        "\n",
        "# Create spatial visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(18, 12))\n",
        "fig.suptitle('Crime Spatial Distribution Analysis', fontsize=16, y=1.02)\n",
        "\n",
        "# 1. Area distribution (top 15)\n",
        "top_15_areas = df['AREA NAME'].value_counts().head(15)\n",
        "axes[0, 0].barh(range(len(top_15_areas)), top_15_areas.values, color='lightblue', alpha=0.8)\n",
        "axes[0, 0].set_yticks(range(len(top_15_areas)))\n",
        "axes[0, 0].set_yticklabels(top_15_areas.index)\n",
        "axes[0, 0].set_title('Top 15 Crime Areas')\n",
        "axes[0, 0].set_xlabel('Number of Crimes')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "sample_size = min(200000, len(df))\n",
        "df_sample_viz = df.sample(n=sample_size, random_state=42)\n",
        "scatter = axes[0, 1].scatter(df_sample_viz['LON'], df_sample_viz['LAT'], \n",
        "                           alpha=0.6, s=1, c='red')\n",
        "axes[0, 1].set_title(f'Crime Locations Scatter Plot (Sample: {sample_size:,})')\n",
        "axes[0, 1].set_xlabel('Longitude')\n",
        "axes[0, 1].set_ylabel('Latitude')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Area vs Crime Type heatmap (top areas and crime types)\n",
        "top_areas = df['AREA NAME'].value_counts().head(10).index\n",
        "top_crimes = df['Crm Cd Desc'].value_counts().head(10).index\n",
        "area_crime_matrix = pd.crosstab(df['AREA NAME'], df['Crm Cd Desc'])\n",
        "area_crime_subset = area_crime_matrix.loc[top_areas, top_crimes]\n",
        "\n",
        "sns.heatmap(area_crime_subset, annot=True, fmt='d', cmap='YlOrRd', \n",
        "            ax=axes[1, 0], cbar_kws={'shrink': 0.5})\n",
        "axes[1, 0].set_title('Area vs Crime Type Heatmap (Top 10 each)')\n",
        "axes[1, 0].set_xlabel('Crime Type')\n",
        "axes[1, 0].set_ylabel('Area')\n",
        "axes[1, 0].tick_params(axis='x', rotation=90)\n",
        "axes[1, 0].tick_params(axis='y', rotation=0)\n",
        "\n",
        "# 4. Crime density by coordinates (2D histogram)\n",
        "axes[1, 1].hist2d(df_sample_viz['LON'], df_sample_viz['LAT'], bins=50, cmap='hot')\n",
        "axes[1, 1].set_title('Crime Density Heatmap (2D Histogram)')\n",
        "axes[1, 1].set_xlabel('Longitude')\n",
        "axes[1, 1].set_ylabel('Latitude')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Spatial insights\n",
        "print(f\"\\nKEY SPATIAL INSIGHTS:\")\n",
        "print(\"-\" * 30)\n",
        "print(f\"Highest crime area: {area_crimes.index[0]} ({area_crimes.iloc[0]:,} crimes)\")\n",
        "print(f\"Lowest crime area: {df['AREA NAME'].value_counts().index[-1]} ({df['AREA NAME'].value_counts().iloc[-1]:,} crimes)\")\n",
        "print(f\"Crime concentration: Top 5 areas account for {(area_crimes.head(5).sum()/len(df)*100):.1f}% of all crimes\")\n",
        "print(f\"Geographic spread: {df['AREA NAME'].nunique()} distinct areas covered\")\n",
        "\n",
        "# Area statistics\n",
        "area_stats = df.groupby('AREA NAME').size().describe()\n",
        "print(f\"\\nArea Crime Statistics:\")\n",
        "print(f\"   Mean crimes per area: {area_stats['mean']:.1f}\")\n",
        "print(f\"   Median crimes per area: {area_stats['50%']:.1f}\")\n",
        "print(f\"   Std deviation: {area_stats['std']:.1f}\")\n",
        "print(f\"   Range: {area_stats['min']:.0f} - {area_stats['max']:.0f} crimes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Crime Type Analysis - Understanding Criminal Activities\n",
        "\n",
        "This section analyzes the different types of crimes and their characteristics:\n",
        "- **Crime categories** - Most common vs rare crimes\n",
        "- **Crime severity** - Part 1 vs Part 2 classifications\n",
        "- **Crime patterns** - Which crimes occur together\n",
        "- **Crime trends** - Changes in crime types over time\n",
        "- **Crime-location relationships** - Where specific crimes happen\n",
        "\n",
        "### 5.1 Crime Type Distribution and Severity Analysis\n",
        "\n",
        "We examine the distribution of different crime types and analyze their severity classifications.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Crime Type Analysis\n",
        "print(\"CRIME TYPE ANALYSIS - UNDERSTANDING CRIMINAL ACTIVITIES\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Basic crime type statistics\n",
        "print(\"Crime Type Overview:\")\n",
        "print(f\"   Total unique crime types: {df['Crm Cd Desc'].nunique()}\")\n",
        "print(f\"   Total unique crime codes: {df['Crm Cd'].nunique()}\")\n",
        "print(f\"   Crime severity distribution:\")\n",
        "severity_dist = df['Part 1-2'].value_counts().sort_index()\n",
        "for part, count in severity_dist.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    severity_name = \"Serious crimes (Part 1)\" if part == 1 else \"Less serious crimes (Part 2)\"\n",
        "    print(f\"     Part {part} - {severity_name}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "# Top crime types analysis\n",
        "print(f\"\\nTop 15 Crime Types:\")\n",
        "top_crimes = df['Crm Cd Desc'].value_counts().head(15)\n",
        "for i, (crime, count) in enumerate(top_crimes.items(), 1):\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"{i:2d}. {crime:<50}: {count:>6,} ({percentage:>5.1f}%)\")\n",
        "\n",
        "# Crime type visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 14))\n",
        "fig.suptitle('Crime Type Distribution Analysis', fontsize=16, y=1.02)\n",
        "\n",
        "# 1. Top 15 crime types\n",
        "top_15_crimes = df['Crm Cd Desc'].value_counts().head(15)\n",
        "axes[0, 0].barh(range(len(top_15_crimes)), top_15_crimes.values, color='lightcoral', alpha=0.8)\n",
        "axes[0, 0].set_yticks(range(len(top_15_crimes)))\n",
        "axes[0, 0].set_yticklabels([crime[:30] + '...' if len(crime) > 30 else crime for crime in top_15_crimes.index])\n",
        "axes[0, 0].set_title('Top 15 Crime Types')\n",
        "axes[0, 0].set_xlabel('Number of Crimes')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Crime severity distribution\n",
        "severity_labels = ['Part 1 (Serious)', 'Part 2 (Less Serious)']\n",
        "severity_counts = [severity_dist[1], severity_dist[2]]\n",
        "axes[0, 1].pie(severity_counts, labels=severity_labels, autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 1].set_title('Crime Severity Distribution')\n",
        "\n",
        "# 3. Crime type distribution (log scale for better visualization)\n",
        "crime_counts = df['Crm Cd Desc'].value_counts()\n",
        "axes[1, 0].hist(crime_counts, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[1, 0].set_title('Crime Type Frequency Distribution')\n",
        "axes[1, 0].set_xlabel('Number of Crimes per Type')\n",
        "axes[1, 0].set_ylabel('Number of Crime Types')\n",
        "axes[1, 0].set_yscale('log')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Crime types by severity\n",
        "part1_crimes = df[df['Part 1-2'] == 1]['Crm Cd Desc'].value_counts().head(10)\n",
        "part2_crimes = df[df['Part 1-2'] == 2]['Crm Cd Desc'].value_counts().head(10)\n",
        "\n",
        "y_pos = np.arange(len(part1_crimes))\n",
        "axes[1, 1].barh(y_pos, part1_crimes.values, alpha=0.8, label='Part 1 (Serious)', color='red')\n",
        "axes[1, 1].set_yticks(y_pos)\n",
        "axes[1, 1].set_yticklabels([crime[:25] + '...' if len(crime) > 25 else crime for crime in part1_crimes.index])\n",
        "axes[1, 1].set_title('Top 10 Part 1 (Serious) Crimes')\n",
        "axes[1, 1].set_xlabel('Number of Crimes')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Crime type insights\n",
        "print(f\"\\nKEY CRIME TYPE INSIGHTS:\")\n",
        "print(\"-\" * 35)\n",
        "print(f\"Most common crime: {top_crimes.index[0]} ({top_crimes.iloc[0]:,} cases)\")\n",
        "print(f\"Least common crime: {df['Crm Cd Desc'].value_counts().index[-1]} ({df['Crm Cd Desc'].value_counts().iloc[-1]:,} cases)\")\n",
        "print(f\"Crime concentration: Top 10 crimes account for {(top_crimes.head(10).sum()/len(df)*100):.1f}% of all crimes\")\n",
        "print(f\"Crime diversity: {df['Crm Cd Desc'].nunique()} different crime types\")\n",
        "\n",
        "# Class imbalance analysis\n",
        "print(f\"\\nClass Imbalance Analysis:\")\n",
        "crime_counts_stats = df['Crm Cd Desc'].value_counts()\n",
        "print(f\"   Max crimes per type: {crime_counts_stats.max():,}\")\n",
        "print(f\"   Min crimes per type: {crime_counts_stats.min():,}\")\n",
        "print(f\"   Mean crimes per type: {crime_counts_stats.mean():.1f}\")\n",
        "print(f\"   Median crimes per type: {crime_counts_stats.median():.1f}\")\n",
        "print(f\"   Imbalance ratio: {crime_counts_stats.max() / crime_counts_stats.min():.1f}:1\")\n",
        "\n",
        "# Rare vs common crimes\n",
        "rare_threshold = 100  # crimes with less than 100 occurrences\n",
        "rare_crimes = crime_counts_stats[crime_counts_stats < rare_threshold]\n",
        "common_crimes = crime_counts_stats[crime_counts_stats >= rare_threshold]\n",
        "\n",
        "print(f\"\\nRare vs Common Crimes Analysis:\")\n",
        "print(f\"   Rare crimes (<{rare_threshold} cases): {len(rare_crimes)} types ({len(rare_crimes)/len(crime_counts_stats)*100:.1f}%)\")\n",
        "print(f\"   Common crimes (≥{rare_threshold} cases): {len(common_crimes)} types ({len(common_crimes)/len(crime_counts_stats)*100:.1f}%)\")\n",
        "print(f\"   Rare crimes total cases: {rare_crimes.sum():,} ({rare_crimes.sum()/len(df)*100:.1f}%)\")\n",
        "print(f\"   Common crimes total cases: {common_crimes.sum():,} ({common_crimes.sum()/len(df)*100:.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Victim Demographics Analysis\n",
        "\n",
        "Understanding victim demographics provides insights into:\n",
        "- **Age patterns** - Which age groups are most vulnerable\n",
        "- **Gender distribution** - Male vs female victim patterns\n",
        "- **Descent analysis** - Ethnic and racial patterns\n",
        "- **Demographic correlations** - How demographics relate to crime types\n",
        "- **Vulnerability assessment** - Identifying at-risk populations\n",
        "\n",
        "### 6.1 Basic Victim Demographics Overview\n",
        "\n",
        "This section provides an overview of victim demographics across the dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Victim Demographics Analysis\n",
        "print(\"VICTIM DEMOGRAPHICS ANALYSIS\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(f\"Demographic Data Overview:\")\n",
        "print(f\"   Total records: {len(df):,}\")\n",
        "print(f\"   Age data: {df['Vict Age'].notna().sum():,} records\")\n",
        "print(f\"   Gender data: {df['Vict Sex'].notna().sum():,} records\")\n",
        "print(f\"   Descent data: {df['Vict Descent'].notna().sum():,} records\")\n",
        "\n",
        "# Age analysis \n",
        "print(f\"\\nAge Analysis:\")\n",
        "valid_ages = df[df['Vict Age'] > -5]['Vict Age']\n",
        "print(f\"   Valid age range: {valid_ages.min():.0f} - {valid_ages.max():.0f} years\")\n",
        "print(f\"   Mean age: {valid_ages.mean():.1f} years\")\n",
        "print(f\"   Median age: {valid_ages.median():.1f} years\")\n",
        "print(f\"   Most common age: {valid_ages.mode().iloc[0]:.0f} years\")\n",
        "\n",
        "# Create age groups (handling special values)\n",
        "def categorize_age(age):\n",
        "    if age == -1:\n",
        "        return 'Not Applicable'\n",
        "    elif age < 0:\n",
        "        return 'Invalid/Missing'\n",
        "    elif age < 18:\n",
        "        return 'Minor (0-17)'\n",
        "    elif age < 25:\n",
        "        return 'Young Adult (18-24)'\n",
        "    elif age < 35:\n",
        "        return 'Adult (25-34)'\n",
        "    elif age < 50:\n",
        "        return 'Middle Age (35-49)'\n",
        "    elif age < 65:\n",
        "        return 'Mature (50-64)'\n",
        "    else:\n",
        "        return 'Senior (65+)'\n",
        "\n",
        "df['age_group'] = df['Vict Age'].apply(categorize_age)\n",
        "\n",
        "# Gender analysis (including all categories)\n",
        "print(f\"\\nGender Analysis:\")\n",
        "gender_dist = df['Vict Sex'].value_counts(dropna=False)\n",
        "gender_mapping = {'M': 'Male', 'F': 'Female', 'H': 'Hetero', 'Unknown': 'Unknown'}\n",
        "for gender, count in gender_dist.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    gender_name = gender_mapping.get(gender, str(gender))\n",
        "    print(f\"   {gender_name}: {count:,} ({percentage:.1f}%)\")\n",
        "\n",
        "# Descent analysis (including all categories)\n",
        "print(f\"\\nDescent Analysis:\")\n",
        "descent_dist = df['Vict Descent'].value_counts(dropna=False).head(10)\n",
        "descent_mapping = {\n",
        "    'H': 'Hispanic/Latino',\n",
        "    'W': 'White',\n",
        "    'B': 'Black',\n",
        "    'O': 'Other',\n",
        "    'A': 'Asian',\n",
        "    'Unknown': 'Unknown',\n",
        "    'K': 'Korean',\n",
        "    'F': 'Filipino',\n",
        "    'C': 'Chinese',\n",
        "    'J': 'Japanese',\n",
        "    'V': 'Vietnamese',\n",
        "    'I': 'American Indian',\n",
        "    'P': 'Pacific Islander',\n",
        "    'G': 'Guamanian',\n",
        "    'S': 'Samoan',\n",
        "    'U': 'Hawaiian',\n",
        "    'Z': 'Asian Indian',\n",
        "    'L': 'Laotian',\n",
        "    'T': 'Thai',\n",
        "    'D': 'Cambodian'\n",
        "}\n",
        "\n",
        "print(\"Top 10 Victim Descent Groups:\")\n",
        "for i, (descent, count) in enumerate(descent_dist.items(), 1):\n",
        "    percentage = (count / len(df)) * 100\n",
        "    desc_name = descent_mapping.get(descent, str(descent))\n",
        "    print(f\"{i:2d}. {desc_name:<20}: {count:>6,} ({percentage:>5.1f}%)\")\n",
        "\n",
        "# Demographic visualizations\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "fig.suptitle('Victim Demographics Analysis', fontsize=16, y=1.02)\n",
        "\n",
        "# 1. Age distribution \n",
        "axes[0, 0].hist(valid_ages, bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[0, 0].set_title('Age Distribution (Valid Ages)')\n",
        "axes[0, 0].set_xlabel('Age')\n",
        "axes[0, 0].set_ylabel('Number of Victims')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Age groups \n",
        "age_group_counts = df['age_group'].value_counts()\n",
        "axes[0, 1].pie(age_group_counts.values, labels=age_group_counts.index, autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 1].set_title('Age Groups Distribution')\n",
        "\n",
        "# 3. Gender distribution (including all categories)\n",
        "gender_counts = df['Vict Sex'].value_counts(dropna=False)\n",
        "gender_labels = [gender_mapping.get(x, str(x)) for x in gender_counts.index]\n",
        "axes[0, 2].pie(gender_counts.values, labels=gender_labels, autopct='%1.1f%%', startangle=90)\n",
        "axes[0, 2].set_title('Gender Distribution')\n",
        "\n",
        "# 4. Top 10 descent groups\n",
        "top_10_descent = df['Vict Descent'].value_counts(dropna=False).head(10)\n",
        "descent_labels = [descent_mapping.get(d, str(d)) for d in top_10_descent.index]\n",
        "axes[1, 0].barh(range(len(top_10_descent)), top_10_descent.values, color='lightcoral', alpha=0.8)\n",
        "axes[1, 0].set_yticks(range(len(top_10_descent)))\n",
        "axes[1, 0].set_yticklabels(descent_labels)\n",
        "axes[1, 0].set_title('Top 10 Victim Descent Groups')\n",
        "axes[1, 0].set_xlabel('Number of Victims')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 5. Age vs Gender \n",
        "df_valid = df[(df['Vict Sex'].notna()) & (df['age_group'] != 'Invalid/Missing')]\n",
        "age_gender_crosstab = pd.crosstab(df_valid['age_group'], df_valid['Vict Sex'])\n",
        "age_gender_crosstab.plot(kind='bar', ax=axes[1, 1], alpha=0.8)\n",
        "axes[1, 1].set_title('Age Groups by Gender')\n",
        "axes[1, 1].set_xlabel('Age Group')\n",
        "axes[1, 1].set_ylabel('Number of Victims')\n",
        "axes[1, 1].tick_params(axis='x', rotation=45)\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Age distribution by gender \n",
        "df_male = df[(df['Vict Sex'] == 'M') & df['Vict Age'] ]\n",
        "df_female = df[(df['Vict Sex'] == 'F') & df['Vict Age'] ]\n",
        "if len(df_male) > 0:\n",
        "    df_male['Vict Age'].hist(bins=30, alpha=0.7, label='Male', ax=axes[1, 2])\n",
        "if len(df_female) > 0:\n",
        "    df_female['Vict Age'].hist(bins=30, alpha=0.7, label='Female', ax=axes[1, 2])\n",
        "axes[1, 2].set_title('Age Distribution by Gender')\n",
        "axes[1, 2].set_xlabel('Age')\n",
        "axes[1, 2].set_ylabel('Number of Victims')\n",
        "axes[1, 2].legend()\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Demographic insights\n",
        "print(f\"\\nKEY DEMOGRAPHIC INSIGHTS:\")\n",
        "print(\"-\" * 35)\n",
        "most_common_age = valid_ages.mode().iloc[0]\n",
        "most_common_gender_code = gender_dist.index[0]\n",
        "most_common_gender = gender_mapping.get(most_common_gender_code, str(most_common_gender_code))\n",
        "most_common_descent = descent_mapping.get(descent_dist.index[0], str(descent_dist.index[0]))\n",
        "most_common_age_group = age_group_counts.index[0]\n",
        "\n",
        "print(f\"Most common victim age: {most_common_age:.0f} years\")\n",
        "print(f\"Most common victim gender: {most_common_gender} ({gender_dist.iloc[0]:,} cases)\")\n",
        "print(f\"Most common victim descent: {most_common_descent} ({descent_dist.iloc[0]:,} cases)\")\n",
        "print(f\"Most common age group: {most_common_age_group} ({age_group_counts.iloc[0]:,} cases)\")\n",
        "\n",
        "# Statistical analysis\n",
        "print(f\"\\nStatistical Analysis:\")\n",
        "if 'M' in gender_dist.index and 'F' in gender_dist.index:\n",
        "    print(f\"   Gender ratio (M:F): {gender_dist['M']/gender_dist['F']:.2f}:1\")\n",
        "print(f\"   Age standard deviation: {valid_ages.std():.1f} years\")\n",
        "print(f\"   Age IQR: {valid_ages.quantile(0.75) - valid_ages.quantile(0.25):.1f} years\")\n",
        "print(f\"   Descent diversity: {df['Vict Descent'].nunique()} different groups\")\n",
        "\n",
        "# Age group statistics\n",
        "print(f\"\\nAge Group Statistics:\")\n",
        "for age_group, count in age_group_counts.items():\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"   {age_group:<20}: {count:>6,} ({percentage:>5.1f}%)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Correlation Analysis & Feature Relationships\n",
        "\n",
        "This section explores relationships between different features to understand:\n",
        "- **Numerical correlations** - How numerical features relate to each other\n",
        "- **Categorical associations** - Relationships between categorical variables\n",
        "- **Feature importance** - Which features are most predictive\n",
        "- **Interaction effects** - How features work together\n",
        "- **Multicollinearity** - Redundant features that might affect modeling\n",
        "\n",
        "### 7.1 Statistical Correlation Analysis\n",
        "\n",
        "We analyze correlations between numerical features and associations between categorical variables.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Correlation Analysis & Feature Relationships\n",
        "print(\"CORRELATION ANALYSIS & FEATURE RELATIONSHIPS\")\n",
        "print(\"=\"*55)\n",
        "\n",
        "# Select numerical columns for correlation analysis\n",
        "numerical_cols = ['DR_NO', 'TIME OCC', 'AREA', 'Rpt Dist No', 'Part 1-2', 'Crm Cd', \n",
        "                  'Vict Age', 'Premis Cd', 'Weapon Used Cd', 'LAT', 'LON', \n",
        "                  'year', 'month', 'day', 'hour', 'minute']\n",
        "\n",
        "# Filter to only include columns that exist in the dataframe\n",
        "available_numerical_cols = [col for col in numerical_cols if col in df.columns]\n",
        "print(f\"Analyzing {len(available_numerical_cols)} numerical features\")\n",
        "\n",
        "# Calculate correlation matrix\n",
        "correlation_matrix = df[available_numerical_cols].corr()\n",
        "\n",
        "# Create correlation visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
        "fig.suptitle('Feature Correlation Analysis', fontsize=16, y=1.02)\n",
        "\n",
        "# 1. Full correlation heatmap\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, ax=axes[0, 0])\n",
        "axes[0, 0].set_title('Complete Correlation Matrix')\n",
        "axes[0, 0].tick_params(axis='x', rotation=45)\n",
        "axes[0, 0].tick_params(axis='y', rotation=0)\n",
        "\n",
        "# 2. Strong correlations only (|correlation| > 0.3)\n",
        "strong_corr_mask = np.abs(correlation_matrix) > 0.3\n",
        "strong_corr = correlation_matrix.where(strong_corr_mask)\n",
        "sns.heatmap(strong_corr, annot=True, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8}, ax=axes[0, 1])\n",
        "axes[0, 1].set_title('Strong Correlations (|r| > 0.3)')\n",
        "axes[0, 1].tick_params(axis='x', rotation=45)\n",
        "axes[0, 1].tick_params(axis='y', rotation=0)\n",
        "\n",
        "# 3. Correlation with target variable (if using Part 1-2 as target)\n",
        "if 'Part 1-2' in df.columns:\n",
        "    target_corr = correlation_matrix['Part 1-2'].abs().sort_values(ascending=False)\n",
        "    target_corr = target_corr.drop('Part 1-2')  # Remove self-correlation\n",
        "    \n",
        "    axes[1, 0].barh(range(len(target_corr)), target_corr.values, color='lightgreen', alpha=0.8)\n",
        "    axes[1, 0].set_yticks(range(len(target_corr)))\n",
        "    axes[1, 0].set_yticklabels(target_corr.index)\n",
        "    axes[1, 0].set_title('Correlation with Crime Severity (Part 1-2)')\n",
        "    axes[1, 0].set_xlabel('Absolute Correlation')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Distribution of correlation values\n",
        "corr_values = correlation_matrix.values\n",
        "corr_values = corr_values[np.triu_indices_from(corr_values, k=1)]  # Upper triangle only\n",
        "axes[1, 1].hist(corr_values, bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
        "axes[1, 1].set_title('Distribution of Correlation Values')\n",
        "axes[1, 1].set_xlabel('Correlation Coefficient')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Find and display highly correlated pairs\n",
        "print(f\"\\nHIGHLY CORRELATED FEATURE PAIRS:\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "high_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_value = correlation_matrix.iloc[i, j]\n",
        "        if abs(corr_value) > 0.7:  # High correlation threshold\n",
        "            high_corr_pairs.append((correlation_matrix.columns[i], \n",
        "                                   correlation_matrix.columns[j], \n",
        "                                   corr_value))\n",
        "\n",
        "if high_corr_pairs:\n",
        "    print(\"Strong correlations (|r| > 0.7):\")\n",
        "    for col1, col2, corr in sorted(high_corr_pairs, key=lambda x: abs(x[2]), reverse=True):\n",
        "        print(f\"  {col1} ↔ {col2}: {corr:.3f}\")\n",
        "else:\n",
        "    print(\"No highly correlated pairs found (|r| > 0.7)\")\n",
        "\n",
        "# Moderate correlations\n",
        "moderate_corr_pairs = []\n",
        "for i in range(len(correlation_matrix.columns)):\n",
        "    for j in range(i+1, len(correlation_matrix.columns)):\n",
        "        corr_value = correlation_matrix.iloc[i, j]\n",
        "        if 0.3 < abs(corr_value) <= 0.7:  # Moderate correlation threshold\n",
        "            moderate_corr_pairs.append((correlation_matrix.columns[i], \n",
        "                                      correlation_matrix.columns[j], \n",
        "                                      corr_value))\n",
        "\n",
        "if moderate_corr_pairs:\n",
        "    print(f\"\\nModerate correlations (0.3 < |r| ≤ 0.7):\")\n",
        "    for col1, col2, corr in sorted(moderate_corr_pairs, key=lambda x: abs(x[2]), reverse=True)[:10]:\n",
        "        print(f\"  {col1} ↔ {col2}: {corr:.3f}\")\n",
        "\n",
        "# Categorical associations analysis\n",
        "print(f\"\\n CATEGORICAL ASSOCIATIONS:\")\n",
        "print(\"-\" * 30)\n",
        "\n",
        "# Chi-square test for categorical independence\n",
        "categorical_pairs = [\n",
        "    ('AREA NAME', 'Crm Cd Desc'),\n",
        "    ('time_category', 'Crm Cd Desc'),\n",
        "    ('Vict Sex', 'Crm Cd Desc'),\n",
        "    ('season', 'Crm Cd Desc'),\n",
        "    ('day_name', 'Crm Cd Desc')\n",
        "]\n",
        "\n",
        "for cat1, cat2 in categorical_pairs:\n",
        "    if cat1 in df.columns and cat2 in df.columns:\n",
        "        # Create contingency table\n",
        "        contingency_table = pd.crosstab(df[cat1], df[cat2])\n",
        "        \n",
        "        # Perform chi-square test\n",
        "        try:\n",
        "            chi2, p_value, dof, expected = chi2_contingency(contingency_table)\n",
        "            \n",
        "            # Calculate Cramér's V (effect size)\n",
        "            n = contingency_table.sum().sum()\n",
        "            cramers_v = np.sqrt(chi2 / (n * (min(contingency_table.shape) - 1)))\n",
        "            \n",
        "            print(f\"{cat1} vs {cat2}:\")\n",
        "            print(f\"  Chi-square: {chi2:.2f}, p-value: {p_value:.2e}\")\n",
        "            print(f\"  Cramér's V: {cramers_v:.3f} ({'Strong' if cramers_v > 0.3 else 'Moderate' if cramers_v > 0.1 else 'Weak'} association)\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            print(f\"{cat1} vs {cat2}: Error in chi-square test\")\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"\\nCORRELATION SUMMARY:\")\n",
        "print(\"-\" * 25)\n",
        "print(f\"  Total feature pairs analyzed: {len(corr_values)}\")\n",
        "print(f\"  High correlations (|r| > 0.7): {len(high_corr_pairs)}\")\n",
        "print(f\"  Moderate correlations (0.3 < |r| ≤ 0.7): {len(moderate_corr_pairs)}\")\n",
        "print(f\"  Mean absolute correlation: {np.mean(np.abs(corr_values)):.3f}\")\n",
        "print(f\"  Max absolute correlation: {np.max(np.abs(corr_values)):.3f}\")\n",
        "print(f\"  Features with high variance: {len([col for col in available_numerical_cols if df[col].std() > df[col].mean()])}\")\n",
        "\n",
        "print(f\"\\n MULTICOLLINEARITY WARNING:\")\n",
        "print(\"-\" * 30)\n",
        "if len(high_corr_pairs) > 0:\n",
        "    print(\"  High correlations detected! Consider:\")\n",
        "    print(\"  - Removing redundant features\")\n",
        "    print(\"  - Using PCA or other dimensionality reduction\")\n",
        "    print(\"  - Regularization techniques in modeling\")\n",
        "else:\n",
        "    print(\"  No severe multicollinearity detected\")\n",
        "    print(\"  Features appear to be relatively independent\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Data Quality Assessment and Imputation\n",
        "\n",
        "This section addresses data quality issues and implements appropriate imputation strategies:\n",
        "- **Missing value patterns** - Understanding what data is missing and why\n",
        "- **Logical imputation** - Using domain knowledge for intelligent imputation\n",
        "- **Weapon usage analysis** - Categorizing crimes by weapon involvement\n",
        "- **Victim age cleaning** - Handling invalid or inconsistent age values\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "### 8.1 Weapon Usage Classification and Imputation\n",
        "\n",
        "We categorize crimes based on whether they typically involve weapons and use this information for intelligent imputation of missing weapon data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crimes_lists import weapon_crimes, non_weapon_crimes\n",
        "\n",
        "print(f\"   Weapon-expected crimes: {len(weapon_crimes)} types\")\n",
        "print(f\"   Non-weapon crimes: {len(non_weapon_crimes)} types\")\n",
        "print(f\"   Total classified: {len(weapon_crimes) + len(non_weapon_crimes)} types\")\n",
        "print(f\"   Total unique crimes in dataset: {df['Crm Cd Desc'].nunique()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"WEAPON IMPUTATION\")\n",
        "print(\"=\" * 80)\n",
        "print(\"Implementing logic-based imputation for 'Weapon Used Cd' column\")\n",
        "print()\n",
        "\n",
        "all_crimes = set(df['Crm Cd Desc'].unique())\n",
        "categorized_crimes = non_weapon_crimes.union(weapon_crimes)  \n",
        "\n",
        "print(f\"Total unique crimes in dataset: {len(all_crimes)}\")\n",
        "print(f\"Crimes categorized: {len(categorized_crimes)}\")\n",
        "print(f\"Coverage: {len(categorized_crimes)/len(all_crimes)*100:.1f}%\")\n",
        "\n",
        "#  Apply the imputation logic\n",
        "# Create a copy of the dataframe for the imputation\n",
        "df_imputed = df.copy()\n",
        "\n",
        "# Count missing values before imputation\n",
        "missing_before = df_imputed['Weapon Used Cd'].isna().sum()\n",
        "print(f\"Missing values before imputation: {missing_before:,} ({missing_before/len(df_imputed)*100:.1f}%)\")\n",
        "\n",
        "# Apply imputation logic\n",
        "def impute_weapon_cd(row):\n",
        "    if pd.notna(row['Weapon Used Cd']):\n",
        "        return row['Weapon Used Cd']  \n",
        "    \n",
        "    crime_type = row['Crm Cd Desc']\n",
        "    \n",
        "    if crime_type in weapon_crimes:\n",
        "        return 999.9  # For weapon crimes with missing weapon info\n",
        "    else:\n",
        "        return 111.1  # For non-weapon crimes or uncategorized crimes\n",
        "\n",
        "# Apply the imputation\n",
        "df_imputed['Weapon Used Cd'] = df_imputed.apply(impute_weapon_cd, axis=1)\n",
        "\n",
        "# Count missing values after imputation\n",
        "missing_after = df_imputed['Weapon Used Cd'].isna().sum()\n",
        "print(f\"Missing values after imputation: {missing_after:,} ({missing_after/len(df_imputed)*100:.1f}%)\")\n",
        "\n",
        "# Show imputation results\n",
        "imputation_results = df_imputed['Weapon Used Cd'].value_counts()\n",
        "print(f\"\\nImputation results:\")\n",
        "print(f\"  • 'NO WEAPON USED': {imputation_results.get('NO WEAPON USED', 0):,} cases\")\n",
        "print(f\"  • 'UNSPECIFIED': {imputation_results.get('UNSPECIFIED', 0):,} cases\")\n",
        "print(f\"  • Original weapon codes: {len(imputation_results) - 2:,} different codes\")\n",
        "\n",
        "print()\n",
        "\n",
        "# Update the main dataframe\n",
        "df = df_imputed.copy()\n",
        "\n",
        "print(f\"Imputed {missing_before:,} missing values using crime-type logic\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean weapon imputation implementation\n",
        "print(\"Weapon Imputation\")\n",
        "\n",
        "# Apply imputation logic\n",
        "df_imputed = df.copy()\n",
        "missing_before = df_imputed['Weapon Used Cd'].isna().sum()\n",
        "\n",
        "def impute_weapon_cd(row):\n",
        "    if pd.notna(row['Weapon Used Cd']):\n",
        "        return row['Weapon Used Cd']\n",
        "    \n",
        "    crime_type = row['Crm Cd Desc']\n",
        "    \n",
        "    if crime_type in weapon_crimes:\n",
        "        return 'UNSPECIFIED'\n",
        "    else:\n",
        "        return 'NO WEAPON USED'\n",
        "\n",
        "df_imputed['Weapon Used Cd'] = df_imputed.apply(impute_weapon_cd, axis=1)\n",
        "missing_after = df_imputed['Weapon Used Cd'].isna().sum()\n",
        "\n",
        "print(f\"\\nImputation results:\")\n",
        "print(f\"Missing values before: {missing_before:,} ({missing_before/len(df_imputed)*100:.1f}%)\")\n",
        "print(f\"Missing values after: {missing_after:,}\")\n",
        "\n",
        "imputation_results = df_imputed['Weapon Used Cd'].value_counts()\n",
        "print(f\"'NO WEAPON USED': {imputation_results.get('NO WEAPON USED', 0):,} cases\")\n",
        "print(f\"'UNSPECIFIED': {imputation_results.get('UNSPECIFIED', 0):,} cases\")\n",
        "\n",
        "# Update main dataframe\n",
        "df = df_imputed.copy()\n",
        "print(\"Weapon imputation completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Applying the same logic to 'Weapon Desc' column for consistency\")\n",
        "\n",
        "# Apply the same logic to 'Weapon Desc' column\n",
        "def impute_weapon_desc(row):\n",
        "    if pd.notna(row['Weapon Desc']):\n",
        "        return row['Weapon Desc']  \n",
        "    \n",
        "    crime_type = row['Crm Cd Desc']\n",
        "    \n",
        "    if crime_type in weapon_crimes:\n",
        "        return 'UNSPECIFIED'  \n",
        "    else:\n",
        "        return 'NO WEAPON USED' \n",
        "\n",
        "# Apply the imputation to Weapon Desc\n",
        "df['Weapon Desc'] = df.apply(impute_weapon_desc, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a is_used_weapon to indicate if the crime involved a weapon\n",
        "df['is_used_weapon'] = (df['Weapon Desc'] != 'NO WEAPON USED').astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.2 Victim Demographics Imputation\n",
        "\n",
        "This section implements intelligent imputation for victim sex and descent information:\n",
        "- Converting coded missing values to standardized formats\n",
        "- Using crime-type-based imputation for missing demographics\n",
        "- Ensuring data consistency across victim characteristics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_victim_age(df):\n",
        "   \n",
        "    df['Vict Age'] = df['Vict Age'].replace(range(-5, 1), -1)    \n",
        "    return df\n",
        "\n",
        "df = clean_victim_age(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from crimes_lists import human_victim_crimes, not_human_victim_crimes\n",
        "\n",
        "def categorize_crime_victim_type(crime_desc):\n",
        "  \n",
        "    crime_desc_upper = crime_desc.upper()\n",
        "    \n",
        "    # Check for human victim crimes\n",
        "    for crime_type in human_victim_crimes:\n",
        "        if crime_type.upper() in crime_desc_upper:\n",
        "            return 1\n",
        "    \n",
        "    # Check for non-human victim crimes\n",
        "    for crime_type in not_human_victim_crimes:\n",
        "        if crime_type.upper() in crime_desc_upper:\n",
        "            return 0\n",
        "    \n",
        "    # Default to human victim if unclear\n",
        "    return 'human_victim'\n",
        "\n",
        "def fill_vict_age_outliers(df):\n",
        "    \"\"\"\n",
        "    Fill Vict Age outliers (-4 to 0) based on crime type:\n",
        "    - Human victim crimes: fill with -5 (for future appropriate age filling)\n",
        "    - Non-human victim crimes: fill with -1 (not applicable)\n",
        "    \"\"\"\n",
        "    # Create a copy to avoid modifying original\n",
        "    df_cleaned = df.copy()\n",
        "    \n",
        "    # Categorize crimes by victim type\n",
        "    df_cleaned['is_human_victim'] = df_cleaned['Crm Cd Desc'].apply(categorize_crime_victim_type)\n",
        "    \n",
        "    # Identify outliers (ages -4 to 0)\n",
        "    outlier_mask = (df_cleaned['Vict Age'] >= -4) & (df_cleaned['Vict Age'] <= 0)\n",
        "   \n",
        "    # Fill outliers based on crime type\n",
        "    # Human victim crimes: fill with -5\n",
        "    human_victim_outliers = outlier_mask & (df_cleaned['is_human_victim'] == 1)\n",
        "    df_cleaned.loc[human_victim_outliers, 'Vict Age'] = -5\n",
        "    \n",
        "    # Non-human victim crimes: fill with -1\n",
        "    not_human_victim_outliers = outlier_mask & (df_cleaned['is_human_victim'] == 0)\n",
        "    df_cleaned.loc[not_human_victim_outliers, 'Vict Age'] = -1\n",
        "            \n",
        "    return df_cleaned\n",
        "\n",
        "# Apply the function\n",
        "df_cleaned = fill_vict_age_outliers(df)\n",
        "df = df_cleaned"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Replace -5 values with most frequent age by crime type\n",
        "# Get crime types that have -5 values\n",
        "crimes_with_minus_five = df[df['Vict Age'] == -5]['Crm Cd Desc'].value_counts()\n",
        "print(f\"\\nCrime types with -5 values:\")\n",
        "print(crimes_with_minus_five)\n",
        "    \n",
        "# For each crime type with -5 values, find the most frequent age (excluding -5 and -1)\n",
        "replacement_ages = {}\n",
        "    \n",
        "    \n",
        "for crime_type in crimes_with_minus_five.index:\n",
        "    # Get all ages for this crime type, excluding -5 and -1\n",
        "    valid_ages = df[(df['Crm Cd Desc'] == crime_type) & \n",
        "                    (df['Vict Age'] > 0)]['Vict Age']\n",
        "        \n",
        "    if len(valid_ages) > 0:\n",
        "        # Get the most frequent age (mode)\n",
        "        mode_age = valid_ages.mode()\n",
        "        if len(mode_age) > 0:\n",
        "            replacement_age = mode_age.iloc[0]\n",
        "            replacement_ages[crime_type] = replacement_age\n",
        "                \n",
        "        else:\n",
        "            # If no mode found, use median\n",
        "            replacement_age = int(valid_ages.median())\n",
        "            replacement_ages[crime_type] = replacement_age\n",
        "\n",
        "    else:\n",
        "        # If no valid ages found, use overall dataset mode\n",
        "        overall_mode = df[df['Vict Age'] > 0]['Vict Age'].mode()\n",
        "        if len(overall_mode) > 0:\n",
        "            replacement_age = overall_mode.iloc[0]\n",
        "            replacement_ages[crime_type] = replacement_age\n",
        "        \n",
        "    # Apply the replacements\n",
        "    for crime_type, replacement_age in replacement_ages.items():\n",
        "        mask = (df['Crm Cd Desc'] == crime_type) & (df['Vict Age'] == -5)\n",
        "        count_replaced = mask.sum()\n",
        "        df.loc[mask, 'Vict Age'] = replacement_age\n",
        "     \n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive victim demographics imputation\n",
        "def handle_victim_demographics(df):\n",
        "    df_processed = df.copy()\n",
        "    \n",
        "    # Step 1: Handle X values\n",
        "    df_processed['Vict Sex'] = df_processed['Vict Sex'].replace('X', 'Unknown')\n",
        "    df_processed['Vict Descent'] = df_processed['Vict Descent'].replace('X', 'Unknown')\n",
        "    \n",
        "    # Step 2: Crime-type-based imputation for missing values\n",
        "    # Get the most common demographics per crime type\n",
        "    crime_demographics = df_processed.groupby('Crm Cd Desc').agg({\n",
        "        'Vict Sex': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 and not pd.isna(x.mode().iloc[0]) else 'Unknown',\n",
        "        'Vict Descent': lambda x: x.mode().iloc[0] if len(x.mode()) > 0 and not pd.isna(x.mode().iloc[0]) else 'Unknown'\n",
        "    }).to_dict()\n",
        "    \n",
        "    # Apply imputation\n",
        "    for idx, row in df_processed.iterrows():\n",
        "        if pd.isna(row['Vict Sex']):\n",
        "            crime_type = row['Crm Cd Desc']\n",
        "            df_processed.at[idx, 'Vict Sex'] = crime_demographics['Vict Sex'].get(crime_type, 'Unknown')\n",
        "        \n",
        "        if pd.isna(row['Vict Descent']):\n",
        "            crime_type = row['Crm Cd Desc']\n",
        "            df_processed.at[idx, 'Vict Descent'] = crime_demographics['Vict Descent'].get(crime_type, 'Unknown')\n",
        "    \n",
        "    # Step 3: Final cleanup - any remaining NaN to 'Unknown'\n",
        "    df_processed['Vict Sex'].fillna('Unknown', inplace=True)\n",
        "    df_processed['Vict Descent'].fillna('Unknown', inplace=True)\n",
        "    \n",
        "    return df_processed\n",
        "\n",
        "# Apply the processing\n",
        "df_processed = handle_victim_demographics(df)\n",
        "\n",
        "df = df_processed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.3 Date and Time Formatting\n",
        "\n",
        "Standardizing date and time formats for consistency and proper analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['Date Rptd'] = pd.to_datetime(df['Date Rptd'], errors='coerce')\n",
        "df['DATE OCC'] = pd.to_datetime(df['DATE OCC'], errors='coerce')\n",
        "\n",
        "# Make sure that TIME OCC is a 4-character string.\n",
        "df['TIME_OCC_PAD'] = df['TIME OCC'].astype(str).str.zfill(4)\n",
        "\n",
        "# Convert to 24-hour time\n",
        "# df['TIME_OCC_24'] = pd.to_datetime(df['TIME_OCC_PAD'], format='%H%M').dt.time\n",
        "df['TIME_OCC_24'] = pd.to_datetime(df['TIME_OCC_PAD'], format='%H%M', errors='coerce').dt.strftime('%H:%M')\n",
        "\n",
        "df['TIME OCC'] = df['TIME_OCC_24']\n",
        "df.drop(columns='TIME_OCC_24', inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.4 Premises Data Quality Assessment\n",
        "\n",
        "Analyzing and handling missing premises information to maintain data completeness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_no_id = df.drop(columns=['DR_NO'])\n",
        "df_no_id.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df.drop_duplicates(subset=df.columns.difference(['DR_NO']))\n",
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_premis_cd = df['Premis Cd'].isna()\n",
        "missing_premis_desc = df['Premis Desc'].isna()\n",
        "\n",
        "combined_missing = pd.DataFrame({\n",
        "    'Premis Cd Missing': missing_premis_cd,\n",
        "    'Premis Desc Missing': missing_premis_desc\n",
        "})\n",
        "\n",
        "summary = combined_missing.value_counts().reset_index()\n",
        "summary.columns = ['Premis Cd Missing', 'Premis Desc Missing', 'Count']\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "premis_map = df.groupby('Premis Cd')['Premis Desc']\\\n",
        "               .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else None).to_dict()\n",
        "\n",
        "df['Premis Desc'] = df.apply(\n",
        "    lambda row: premis_map.get(row['Premis Cd']) if pd.isna(row['Premis Desc']) else row['Premis Desc'],\n",
        "    axis=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "missing_premis_cd = df['Premis Cd'].isna()\n",
        "missing_premis_desc = df['Premis Desc'].isna()\n",
        "\n",
        "combined_missing = pd.DataFrame({\n",
        "    'Premis Cd Missing': missing_premis_cd,\n",
        "    'Premis Desc Missing': missing_premis_desc\n",
        "})\n",
        "\n",
        "summary = combined_missing.value_counts().reset_index()\n",
        "summary.columns = ['Premis Cd Missing', 'Premis Desc Missing', 'Count']\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Codes for which the 'Premis Desc'is missing\n",
        "premis_cd_for_missing_desc = [256.0, 418.0, 975.0, 974.0, 976.0]\n",
        "\n",
        "# Extract rows where 'Premis Cd' is in the list above AND 'Premis Desc' is available\n",
        "matches = df[\n",
        "    df['Premis Cd'].isin(premis_cd_for_missing_desc) &\n",
        "    df['Premis Desc'].notna()\n",
        "]\n",
        "\n",
        "matches[['Premis Cd', 'Premis Desc']].drop_duplicates().sort_values('Premis Cd')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract unique 'Premis Cd' values where 'Premis Desc' is missing\n",
        "premis_cd_for_missing_desc = df[df['Premis Desc'].isna() & df['Premis Cd'].notna()]['Premis Cd'].dropna().unique()\n",
        "\n",
        "pd.DataFrame(premis_cd_for_missing_desc, columns=['Premis Cd (Missing Desc)'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mask_to_drop = ((missing_premis_cd == False) & (missing_premis_desc == True)) | \\\n",
        "               ((missing_premis_cd == True) & (missing_premis_desc == True))\n",
        "\n",
        "df = df[~mask_to_drop].copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.5 Modus Operandi (Mocodes) Analysis and Imputation\n",
        "\n",
        "Analyzing the relationship between Mocodes and other features, and implementing intelligent imputation for missing values.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the number of codes in each 'Mocodes' field\n",
        "df['Mocodes_Count'] = df['Mocodes'].fillna('').apply(lambda x: len(x.strip().split()))\n",
        "\n",
        "numeric_df = df.select_dtypes(include=['int64', 'float64']).copy()\n",
        "numeric_df['Mocodes_Count'] = df['Mocodes_Count']\n",
        "\n",
        "correlation_with_mocodes = numeric_df.corr()['Mocodes_Count'].sort_values(ascending=False)\n",
        "\n",
        "correlation_with_mocodes = correlation_with_mocodes.drop('Mocodes_Count')\n",
        "print(correlation_with_mocodes)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x, y)\n",
        "    chi2 = chi2_contingency(confusion_matrix, correction=False)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2 / n\n",
        "    r, k = confusion_matrix.shape\n",
        "    return np.sqrt(phi2 / min(k - 1, r - 1))\n",
        "\n",
        "categorical_columns = df.select_dtypes(include='object').columns.tolist()\n",
        "\n",
        "# Exclude long text fields or date-related columns\n",
        "categorical_columns = [col for col in categorical_columns if col not in ['Date Rptd', 'DATE OCC', 'LOCATION', 'Cross Street', 'Mocodes']]\n",
        "\n",
        "\n",
        "results = {}\n",
        "for col in categorical_columns:\n",
        "    try:\n",
        "        v = cramers_v(df['Mocodes_Count'], df[col])\n",
        "        results[col] = v\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "pd.Series(results).sort_values(ascending=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a dictionary of the most frequent Mocodes for each crime type\n",
        "mocode_map = (\n",
        "    df.groupby('Crm Cd Desc')['Mocodes']\n",
        "    .agg(lambda x: x.mode().iloc[0] if not x.mode().empty else np.nan)\n",
        "    .to_dict()\n",
        ")\n",
        "\n",
        "# Fill missing Mocodes values based on the crime type\n",
        "df['Mocodes'] = df.apply(\n",
        "    lambda row: mocode_map[row['Crm Cd Desc']] if pd.isna(row['Mocodes']) else row['Mocodes'],\n",
        "    axis=1\n",
        ")\n",
        "\n",
        "df['Mocodes'] = df['Mocodes'].fillna('noMocode')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_df = df.select_dtypes(include=['int64', 'float64'])\n",
        "\n",
        "correlation_matrix = numeric_df.corr()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.6 Outlier Detection and Analysis\n",
        "\n",
        "Identifying and analyzing outliers in numerical features using statistical methods.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "numeric_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "# IQR\n",
        "def detect_outliers_iqr(data, col):\n",
        "    Q1 = data[col].quantile(0.25)\n",
        "    Q3 = data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
        "\n",
        "outlier_summary = {}\n",
        "for col in numeric_cols:\n",
        "    outliers = detect_outliers_iqr(df, col)\n",
        "    outlier_summary[col] = {\n",
        "        'outlier_count': outliers.shape[0],\n",
        "        'percentage': round((outliers.shape[0] / df.shape[0]) * 100, 2)\n",
        "    }\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_summary).T.sort_values(by='outlier_count', ascending=False)\n",
        "print(\"outliers value\")\n",
        "print(outlier_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.boxplot(x=df[col])\n",
        "    plt.title(f'Boxplot for {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"LAT range:\", df['LAT'].min(), \"-\", df['LAT'].max())\n",
        "print(\"LON range:\", df['LON'].min(), \"-\", df['LON'].max())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df[\n",
        "    (df['LAT'] != 0.0) &\n",
        "    (df['LON'] != 0.0)\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IQR\n",
        "def detect_outliers_iqr(data, col):\n",
        "    Q1 = data[col].quantile(0.25)\n",
        "    Q3 = data[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "    return data[(data[col] < lower_bound) | (data[col] > upper_bound)]\n",
        "\n",
        "outlier_summary = {}\n",
        "for col in numeric_cols:\n",
        "    outliers = detect_outliers_iqr(df, col)\n",
        "    outlier_summary[col] = {\n",
        "        'outlier_count': outliers.shape[0],\n",
        "        'percentage': round((outliers.shape[0] / df.shape[0]) * 100, 2)\n",
        "    }\n",
        "\n",
        "outlier_df = pd.DataFrame(outlier_summary).T.sort_values(by='outlier_count', ascending=False)\n",
        "print(\"outliers value\")\n",
        "print(outlier_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"LAT range:\", df['LAT'].min(), \"-\", df['LAT'].max())\n",
        "print(\"LON range:\", df['LON'].min(), \"-\", df['LON'].max())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.7 Skewness"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for col in numeric_cols:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.histplot(data=df, x=col, bins=30, kde=True)\n",
        "    plt.title(f'Histogram for {col}')\n",
        "    plt.xlabel(col)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "skew_values = df.select_dtypes(include=['int64', 'float64']).skew().sort_values(ascending=False)\n",
        "\n",
        "print(skew_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Columns with high skewness (|skew| > 0.75)\n",
        "df['Mocodes_Count'] = np.log1p(df['Mocodes_Count'])        # Right-skewed → log1p\n",
        "df['Weapon Used Cd'] = np.cbrt(df['Weapon Used Cd'])       # Left-skewed → cbrt\n",
        "df['LON'] = np.cbrt(df['LON'])                             # Close to left-skewed → cbrt\n",
        "\n",
        "#  moderately skewed (|skew| between 0.5 and 0.75)\n",
        "df['Premis Cd'] = np.log1p(df['Premis Cd'])                \n",
        "df['Crm Cd'] = np.log1p(df['Crm Cd'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "skew_values = df.select_dtypes(include=['int64', 'float64']).skew().sort_values(ascending=False)\n",
        "print(skew_values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_weighted_risk_score(df):\n",
        "    \"\"\"Calculate weighted risk score for each area\"\"\"\n",
        "    \n",
        "    # Convert DATE OCC to datetime\n",
        "    df['date_occurred'] = pd.to_datetime(df['DATE OCC'])\n",
        "    max_date = df['date_occurred'].max()\n",
        "    \n",
        "    # Calculate days since crime occurred\n",
        "    df['days_ago'] = (max_date - df['date_occurred']).dt.days\n",
        "    \n",
        "    # Calculate recency weight (exponential decay)\n",
        "    df['recency_weight'] = np.exp(-df['days_ago'] / 365)\n",
        "    \n",
        "    # Calculate severity weight (Part 1 crimes weighted 2x)\n",
        "    df['severity_weight'] = df['Part 1-2'].map({1: 2.0, 2: 1.0})\n",
        "    \n",
        "    # Calculate weighted crime score for each crime\n",
        "    df['crime_score'] = df['recency_weight'] * df['severity_weight']\n",
        "    \n",
        "    # Aggregate by area\n",
        "    area_risk_scores = df.groupby('AREA NAME').agg({\n",
        "        'DR_NO': 'count',\n",
        "        'crime_score': 'sum',\n",
        "        'Part 1-2': lambda x: (x == 1).sum(),\n",
        "        'days_ago': 'mean'\n",
        "    }).reset_index()\n",
        "    \n",
        "    area_risk_scores.columns = ['AREA NAME', 'crime_count', 'weighted_score', 'serious_crimes', 'avg_days_ago']\n",
        "    \n",
        "    # Calculate serious crime ratio\n",
        "    area_risk_scores['serious_crime_ratio'] = area_risk_scores['serious_crimes'] / area_risk_scores['crime_count']\n",
        "    \n",
        "    # Calculate final risk score (normalized)\n",
        "    area_risk_scores['risk_score'] = (\n",
        "        0.7 * (area_risk_scores['weighted_score'] / area_risk_scores['weighted_score'].max()) +\n",
        "        0.3 * area_risk_scores['serious_crime_ratio']\n",
        "    )\n",
        "    \n",
        "    # Scale risk score to 0-100\n",
        "    area_risk_scores['risk_score'] = area_risk_scores['risk_score'] * 100\n",
        "    \n",
        "    return area_risk_scores\n",
        "\n",
        "def add_area_risk_levels(df):\n",
        "    \"\"\"Add risk level columns based on weighted risk scores\"\"\"\n",
        "    \n",
        "    # Calculate risk scores\n",
        "    area_risk_scores = calculate_weighted_risk_score(df)\n",
        "    \n",
        "    # Sort by risk score\n",
        "    area_risk_scores = area_risk_scores.sort_values('risk_score', ascending=False)\n",
        "    \n",
        "    # Create risk level categories using quartiles\n",
        "    area_risk_scores['risk_level'] = pd.qcut(\n",
        "        area_risk_scores['risk_score'], \n",
        "        q=4, \n",
        "        labels=['Low Risk', 'Medium Risk', 'High Risk', 'Very High Risk']\n",
        "    )\n",
        "    \n",
        "    # Create numeric risk level\n",
        "    area_risk_scores['risk_level_numeric'] = pd.qcut(\n",
        "        area_risk_scores['risk_score'], \n",
        "        q=4, \n",
        "        labels=[1, 2, 3, 4]\n",
        "    ).astype(int)\n",
        "    \n",
        "    # Create mapping dictionaries\n",
        "    risk_mapping = dict(zip(area_risk_scores['AREA NAME'], area_risk_scores['risk_level']))\n",
        "    risk_numeric_mapping = dict(zip(area_risk_scores['AREA NAME'], area_risk_scores['risk_level_numeric']))\n",
        "    risk_score_mapping = dict(zip(area_risk_scores['AREA NAME'], area_risk_scores['risk_score']))\n",
        "    \n",
        "    # Add risk level columns to original dataframe\n",
        "    df['area_risk_level'] = df['AREA NAME'].map(risk_mapping)\n",
        "    df['area_risk_level_numeric'] = df['AREA NAME'].map(risk_numeric_mapping)\n",
        "    df['area_risk_score'] = df['AREA NAME'].map(risk_score_mapping)\n",
        "    \n",
        "    # Drop temporary columns if they exist\n",
        "    temp_cols = ['date_occurred', 'days_ago', 'recency_weight', 'severity_weight', 'crime_score']\n",
        "    existing_temp_cols = [col for col in temp_cols if col in df.columns]\n",
        "    if existing_temp_cols:\n",
        "        df = df.drop(columns=existing_temp_cols)\n",
        "    \n",
        "    return df, area_risk_scores\n",
        "\n",
        "# Execute the analysis\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"CALCULATING WEIGHTED AREA RISK LEVELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "df, area_statistics = add_area_risk_levels(df.copy())\n",
        "\n",
        "# Display results\n",
        "print(\"\\n🔥 Top 10 Highest Risk Areas:\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"{'Area':<20} {'Risk Score':<12} {'Crimes':<10} {'Serious %':<12} {'Risk Level':<15}\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "for _, row in area_statistics.head(10).iterrows():\n",
        "    print(f\"{row['AREA NAME']:<20} {row['risk_score']:>10.2f} {row['crime_count']:>10,} \"\n",
        "          f\"{row['serious_crime_ratio']*100:>10.1f}% {row['risk_level']:<15}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 8.8 Final Data Export\n",
        "\n",
        "Saving the cleaned and processed dataset for further analysis and modeling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results\n",
        "output_filename = 'data/cleaned_crime_data.csv'\n",
        "df.to_csv(output_filename, index=False)\n",
        "print(f\"\\n✅ Enhanced dataset saved as: {output_filename}\")\n",
        "\n",
        "area_stats_filename = 'area_weighted_risk_statistics.csv'\n",
        "area_statistics.to_csv(area_stats_filename, index=False)\n",
        "print(f\"✅ Area statistics saved as: {area_stats_filename}\")\n",
        "\n",
        "print(f\"\\n📊 Process completed! Added 3 new columns:\")\n",
        "print(\"- area_risk_level (categorical)\")\n",
        "print(\"- area_risk_level_numeric (numeric 1-4)\")  \n",
        "print(\"- area_risk_score (raw score 0-100)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df.columns.tolist()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
