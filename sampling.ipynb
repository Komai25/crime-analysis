{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4a1f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f71a334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the crime data\n",
    "print(\"Loading crime data...\")\n",
    "df = pd.read_csv('crime_data.csv')\n",
    "\n",
    "print(f\"Dataset loaded successfully!\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nFirst few rows:\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7b4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic dataset information\n",
    "print(\"=\"*50)\n",
    "print(\"DATASET INFORMATION\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset shape: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\nData types:\")\n",
    "print(df.dtypes)\n",
    "print(\"\\nDataset info:\")\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0eedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values analysis\n",
    "print(\"=\"*50)\n",
    "print(\"MISSING VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "missing_values = df.isnull().sum()\n",
    "missing_percent = (missing_values / len(df)) * 100\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Missing Count': missing_values,\n",
    "    'Missing Percentage': missing_percent\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "print(missing_df[missing_df['Missing Count'] > 0])\n",
    "\n",
    "# Visualize missing values\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "missing_df[missing_df['Missing Count'] > 0].plot(x='Column', y='Missing Count', kind='bar')\n",
    "plt.title('Missing Values Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "missing_df[missing_df['Missing Count'] > 0].plot(x='Column', y='Missing Percentage', kind='bar')\n",
    "plt.title('Missing Values Percentage')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd88ad6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical summary for numerical columns\n",
    "print(\"=\"*50)\n",
    "print(\"STATISTICAL SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "categorical_cols = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "print(f\"Numerical columns: {list(numerical_cols)}\")\n",
    "print(f\"Categorical columns: {list(categorical_cols)}\")\n",
    "\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"\\nNumerical columns statistics:\")\n",
    "    print(df[numerical_cols].describe())\n",
    "    \n",
    "if len(categorical_cols) > 0:\n",
    "    print(\"\\nCategorical columns statistics:\")\n",
    "    print(df[categorical_cols].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e075d576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique values analysis\n",
    "print(\"=\"*50)\n",
    "print(\"UNIQUE VALUES ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for col in df.columns:\n",
    "    unique_count = df[col].nunique()\n",
    "    unique_percent = (unique_count / len(df)) * 100\n",
    "    print(f\"{col}: {unique_count} unique values ({unique_percent:.2f}%)\")\n",
    "    \n",
    "    # Show sample values for categorical columns with reasonable number of unique values\n",
    "    if df[col].dtype == 'object' and unique_count <= 20:\n",
    "        print(f\"  Sample values: {df[col].unique()[:10].tolist()}\")\n",
    "    elif df[col].dtype == 'object' and unique_count > 20:\n",
    "        print(f\"  Sample values: {df[col].unique()[:5].tolist()} ... (showing first 5)\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19c95f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data distribution visualizations\n",
    "print(\"=\"*50)\n",
    "print(\"DATA DISTRIBUTION VISUALIZATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Plot distributions for numerical columns\n",
    "if len(numerical_cols) > 0:\n",
    "    n_numerical = len(numerical_cols)\n",
    "    fig, axes = plt.subplots(2, max(2, (n_numerical + 1) // 2), figsize=(15, 8))\n",
    "    fig.suptitle('Distribution of Numerical Columns', fontsize=16)\n",
    "    axes = axes.flatten() if n_numerical > 1 else [axes]\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols):\n",
    "        if i < len(axes):\n",
    "            axes[i].hist(df[col].dropna(), bins=30, alpha=0.7, edgecolor='black')\n",
    "            axes[i].set_title(f'{col} Distribution')\n",
    "            axes[i].set_xlabel(col)\n",
    "            axes[i].set_ylabel('Frequency')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(len(numerical_cols), len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot top categories for categorical columns\n",
    "if len(categorical_cols) > 0:\n",
    "    n_categorical = min(len(categorical_cols), 6)  # Show max 6 categorical columns\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Top Categories in Categorical Columns', fontsize=16)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, col in enumerate(categorical_cols[:n_categorical]):\n",
    "        top_values = df[col].value_counts().head(10)\n",
    "        axes[i].bar(range(len(top_values)), top_values.values)\n",
    "        axes[i].set_title(f'Top 10 values in {col}')\n",
    "        axes[i].set_xlabel(col)\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].set_xticks(range(len(top_values)))\n",
    "        axes[i].set_xticklabels(top_values.index, rotation=45, ha='right')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_categorical, len(axes)):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e1090",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis for numerical columns\n",
    "print(\"=\"*50)\n",
    "print(\"CORRELATION ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "if len(numerical_cols) > 1:\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0,\n",
    "                square=True, linewidths=0.5, cbar_kws={\"shrink\": .5})\n",
    "    plt.title('Correlation Matrix of Numerical Variables')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show highly correlated pairs\n",
    "    print(\"\\nHighly correlated pairs (|correlation| > 0.5):\")\n",
    "    high_corr = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.5:\n",
    "                high_corr.append((correlation_matrix.columns[i], \n",
    "                                correlation_matrix.columns[j], \n",
    "                                corr_value))\n",
    "    \n",
    "    if high_corr:\n",
    "        for col1, col2, corr in high_corr:\n",
    "            print(f\"{col1} - {col2}: {corr:.3f}\")\n",
    "    else:\n",
    "        print(\"No highly correlated pairs found.\")\n",
    "        \n",
    "elif len(numerical_cols) == 1:\n",
    "    print(f\"Only one numerical column found: {numerical_cols[0]}\")\n",
    "else:\n",
    "    print(\"No numerical columns found for correlation analysis.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d902a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment and outlier detection\n",
    "print(\"=\"*50)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicates}\")\n",
    "\n",
    "if duplicates > 0:\n",
    "    print(f\"Percentage of duplicates: {(duplicates/len(df))*100:.2f}%\")\n",
    "\n",
    "# Outlier detection using IQR method for numerical columns\n",
    "if len(numerical_cols) > 0:\n",
    "    print(\"\\nOutlier detection (using IQR method):\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, min(len(numerical_cols), 4), figsize=(15, 5))\n",
    "    if len(numerical_cols) == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols[:4]):  # Show max 4 boxplots\n",
    "        Q1 = df[col].quantile(0.25)\n",
    "        Q3 = df[col].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)][col]\n",
    "        print(f\"{col}: {len(outliers)} outliers ({len(outliers)/len(df)*100:.2f}%)\")\n",
    "        \n",
    "        if i < len(axes):\n",
    "            axes[i].boxplot(df[col].dropna())\n",
    "            axes[i].set_title(f'{col} Boxplot')\n",
    "            axes[i].set_ylabel(col)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Data completeness summary\n",
    "print(\"\\nData completeness summary:\")\n",
    "completeness = ((df.count() / len(df)) * 100).round(2)\n",
    "print(completeness.sort_values(ascending=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"EXPLORATION COMPLETE\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Dataset contains {len(df)} rows and {len(df.columns)} columns\")\n",
    "print(f\"Missing values: {df.isnull().sum().sum()} total\")\n",
    "print(f\"Duplicate rows: {duplicates}\")\n",
    "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caca3489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 1: Drop the identified problematic columns\n",
    "print(\"=\"*50)\n",
    "print(\"COLUMN DROPPING & SAMPLING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "columns_to_drop = ['Crm Cd 1', 'Crm Cd 3', 'Crm Cd 4', 'Crm Cd 2', 'Cross Street']\n",
    "print(f\"Dropping columns: {columns_to_drop}\")\n",
    "\n",
    "# Drop columns\n",
    "df_cleaned = df.drop(columns=columns_to_drop)\n",
    "print(f\"Original shape: {df.shape}\")\n",
    "print(f\"After dropping columns: {df_cleaned.shape}\")\n",
    "\n",
    "# Check memory reduction\n",
    "original_memory = df.memory_usage(deep=True).sum() / 1024**2\n",
    "cleaned_memory = df_cleaned.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Memory reduction: {original_memory:.2f} MB ‚Üí {cleaned_memory:.2f} MB\")\n",
    "\n",
    "print(f\"\\nRemaining columns: {df_cleaned.columns.tolist()}\")\n",
    "print(f\"Number of columns: {len(df_cleaned.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833abb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 2: Comprehensive Stratified Sampling (200k rows)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"STRATIFIED SAMPLING - 200K ROWS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Target sample size\n",
    "target_size = 201000\n",
    "sample_ratio = target_size / len(df_cleaned)\n",
    "print(f\"Sampling ratio: {sample_ratio:.3f} ({sample_ratio*100:.1f}%)\")\n",
    "\n",
    "# Strategy: Use stratified sampling based on key variables to maintain distributions\n",
    "# Choose stratification variable with good distribution\n",
    "stratify_column = 'AREA NAME'  # Good categorical distribution (21 unique values)\n",
    "\n",
    "# Perform stratified sampling\n",
    "df_sample, _ = train_test_split(\n",
    "    df_cleaned, \n",
    "    test_size=1-sample_ratio, \n",
    "    random_state=42,\n",
    "    stratify=df_cleaned[stratify_column]\n",
    ")\n",
    "\n",
    "print(f\"Sample size: {len(df_sample)} rows\")\n",
    "print(f\"Percentage of original data: {len(df_sample)/len(df_cleaned)*100:.2f}%\")\n",
    "\n",
    "# Memory comparison\n",
    "sample_memory = df_sample.memory_usage(deep=True).sum() / 1024**2\n",
    "print(f\"Memory usage: {cleaned_memory:.2f} MB ‚Üí {sample_memory:.2f} MB\")\n",
    "print(f\"Memory reduction: {(1-sample_memory/cleaned_memory)*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055b20b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 3: Validate Sampling Quality - Ensure distributions are maintained\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAMPLING VALIDATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Compare missing value percentages\n",
    "print(\"MISSING VALUES COMPARISON:\")\n",
    "print(\"Column\" + \" \" * 15 + \"Original %\" + \" \" * 5 + \"Sample %\" + \" \" * 5 + \"Difference\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for col in df_sample.columns:\n",
    "    orig_missing = (df_cleaned[col].isnull().sum() / len(df_cleaned)) * 100\n",
    "    sample_missing = (df_sample[col].isnull().sum() / len(df_sample)) * 100\n",
    "    diff = abs(orig_missing - sample_missing)\n",
    "    print(f\"{col:<20} {orig_missing:>8.2f}% {sample_missing:>10.2f}% {diff:>10.2f}%\")\n",
    "\n",
    "# Compare key categorical distributions\n",
    "print(f\"\\nCATEGORICAL DISTRIBUTION COMPARISON:\")\n",
    "print(f\"Stratification variable: {stratify_column}\")\n",
    "print(\"Category\" + \" \" * 15 + \"Original %\" + \" \" * 5 + \"Sample %\" + \" \" * 5 + \"Difference\")\n",
    "print(\"-\" * 65)\n",
    "\n",
    "orig_dist = df_cleaned[stratify_column].value_counts(normalize=True) * 100\n",
    "sample_dist = df_sample[stratify_column].value_counts(normalize=True) * 100\n",
    "\n",
    "for category in orig_dist.index:\n",
    "    orig_pct = orig_dist[category]\n",
    "    sample_pct = sample_dist[category]\n",
    "    diff = abs(orig_pct - sample_pct)\n",
    "    print(f\"{category:<20} {orig_pct:>8.2f}% {sample_pct:>10.2f}% {diff:>10.2f}%\")\n",
    "\n",
    "# Compare basic statistics for numerical columns\n",
    "print(f\"\\nNUMERICAL STATISTICS COMPARISON:\")\n",
    "numerical_cols_cleaned = df_sample.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_cols_cleaned[:5]:  # Show first 5 numerical columns\n",
    "    orig_mean = df_cleaned[col].mean()\n",
    "    sample_mean = df_sample[col].mean()\n",
    "    diff_pct = abs(orig_mean - sample_mean) / orig_mean * 100\n",
    "  \n",
    "    print(f\"{col}: Original mean={orig_mean:.2f}, Sample mean={sample_mean:.2f}, Diff={diff_pct:.2f}%\")\n",
    "\n",
    "print(f\"\\n‚úÖ SAMPLING SUCCESSFUL!\")\n",
    "print(f\"‚úÖ Final dataset: {len(df_sample)} rows, {len(df_sample.columns)} columns\")\n",
    "print(f\"‚úÖ Memory usage: {sample_memory:.2f} MB\")\n",
    "print(f\"‚úÖ Distributions preserved with minimal deviation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abe0070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STEP 4: Save the cleaned and sampled dataset\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"SAVING FINAL DATASET\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = f'crime_data_sample_{target_size}.csv'\n",
    "df_sample.to_csv(output_filename, index=False)\n",
    "print(f\"‚úÖ Dataset saved as: {output_filename}\")\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\nüéØ FINAL DATASET SUMMARY:\")\n",
    "print(f\"üìä Original dataset: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"üóëÔ∏è  Dropped {len(columns_to_drop)} problematic columns\")\n",
    "print(f\"üì¶ Sampled dataset: {len(df_sample):,} rows √ó {len(df_sample.columns)} columns\")\n",
    "print(f\"üíæ Memory usage: {sample_memory:.2f} MB\")\n",
    "print(f\"üìà Data reduction: {(1-len(df_sample)/len(df))*100:.1f}%\")\n",
    "print(f\"üéØ Perfect for ML project - manageable size with preserved distributions!\")\n",
    "\n",
    "# Display final dataset info\n",
    "print(f\"\\nüìã Ready for ML Pipeline:\")\n",
    "print(f\"‚úì Clean dataset with {len(df_sample.columns)} features\")\n",
    "print(f\"‚úì No redundant columns\")\n",
    "print(f\"‚úì Representative sample\")\n",
    "print(f\"‚úì Manageable computational requirements\")\n",
    "print(f\"‚úì Suitable for classification/regression tasks\")\n",
    "\n",
    "# Show first few rows of final dataset\n",
    "print(f\"\\nüîç Preview of final dataset:\")\n",
    "print(df_sample.head())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# ML Project Ideas Analysis\n",
    "\n",
    "Based on the crime dataset analysis and project requirements, here are the recommended ML problems to implement:\n",
    "\n",
    "## 1. üéØ Primary Recommendation: Crime Type Classification\n",
    "\n",
    "**Problem Statement**: Predict the type of crime (`Crm Cd Desc`) based on temporal, spatial, and contextual features.\n",
    "\n",
    "**Why This Problem is Perfect**:\n",
    "- Multi-class classification with 140 unique crime types\n",
    "- Real-world complexity with class imbalance\n",
    "- Rich feature set combining temporal, spatial, and demographic data\n",
    "- High practical importance for law enforcement resource allocation\n",
    "- Meets all project requirements for complexity and non-triviality\n",
    "\n",
    "**Target Variable**: `Crm Cd Desc` (Crime Code Description)\n",
    "**Problem Type**: Multi-class Classification\n",
    "**Evaluation Metrics**: Accuracy, F1-Score, Precision, Recall, Confusion Matrix\n",
    "\n",
    "## 2. üîÑ Alternative: Crime Severity Prediction\n",
    "\n",
    "**Problem Statement**: Predict crime severity (`Part 1-2`) based on available features.\n",
    "\n",
    "**Why This is Also Excellent**:\n",
    "- Binary classification (easier to interpret and explain)\n",
    "- Balanced target variable\n",
    "- High practical value for resource allocation\n",
    "- Clear business impact and interpretability\n",
    "\n",
    "**Target Variable**: `Part 1-2` (1 = Serious crimes, 2 = Less serious crimes)\n",
    "**Problem Type**: Binary Classification\n",
    "**Evaluation Metrics**: Accuracy, F1-Score, Precision, Recall, ROC-AUC\n",
    "\n",
    "## 3. üîß Feature Engineering Opportunities\n",
    "\n",
    "Both problems offer rich feature engineering possibilities:\n",
    "- **Temporal features**: Hour bins, day of week, month, season\n",
    "- **Spatial features**: Area clustering, distance calculations\n",
    "- **Demographic features**: Victim age groups, sex, descent combinations\n",
    "- **Location features**: Premise type grouping, high-risk location indicators\n",
    "- **Interaction features**: Time-location interactions, demographic-crime patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a7d190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ML Project Implementation Plan\n",
    "print(\"=\"*60)\n",
    "print(\"ML PROJECT IMPLEMENTATION ROADMAP\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Let's analyze the target variables for our ML problems\n",
    "print(\"üéØ TARGET VARIABLE ANALYSIS:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "# Problem 1: Crime Type Classification\n",
    "print(\"1. CRIME TYPE CLASSIFICATION:\")\n",
    "print(f\"   Target: Crm Cd Desc\")\n",
    "print(f\"   Classes: {df_sample['Crm Cd Desc'].nunique()} unique crime types\")\n",
    "print(f\"   Top 5 most common crimes:\")\n",
    "top_crimes = df_sample['Crm Cd Desc'].value_counts().head()\n",
    "for crime, count in top_crimes.items():\n",
    "    percentage = (count / len(df_sample)) * 100\n",
    "    print(f\"     - {crime}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n   Class imbalance ratio: {top_crimes.iloc[0] / top_crimes.iloc[-1]:.1f}:1\")\n",
    "print(f\"   Challenge level: HIGH (Multi-class with imbalance)\")\n",
    "\n",
    "# Problem 2: Crime Severity Prediction  \n",
    "print(f\"\\n2. CRIME SEVERITY PREDICTION:\")\n",
    "print(f\"   Target: Part 1-2\")\n",
    "severity_dist = df_sample['Part 1-2'].value_counts().sort_index()\n",
    "print(f\"   Classes: {len(severity_dist)} (Binary classification)\")\n",
    "for severity, count in severity_dist.items():\n",
    "    percentage = (count / len(df_sample)) * 100\n",
    "    severity_name = \"Serious crimes\" if severity == 1 else \"Less serious crimes\"\n",
    "    print(f\"     - Part {severity} ({severity_name}): {count} ({percentage:.1f}%)\")\n",
    "\n",
    "balance_ratio = severity_dist.iloc[0] / severity_dist.iloc[1]\n",
    "print(f\"   Balance ratio: {balance_ratio:.2f}:1\")\n",
    "print(f\"   Challenge level: MEDIUM (Binary with slight imbalance)\")\n",
    "\n",
    "# Problem 3: Weapon Usage Prediction\n",
    "print(f\"\\n3. WEAPON USAGE PREDICTION:\")\n",
    "print(f\"   Target: Weapon Used (derived from Weapon Desc)\")\n",
    "weapon_used = df_sample['Weapon Desc'].notna()\n",
    "weapon_dist = weapon_used.value_counts()\n",
    "print(f\"   Classes: 2 (Binary classification)\")\n",
    "for used, count in weapon_dist.items():\n",
    "    percentage = (count / len(df_sample)) * 100\n",
    "    label = \"Weapon used\" if used else \"No weapon\"\n",
    "    print(f\"     - {label}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "weapon_ratio = weapon_dist.iloc[0] / weapon_dist.iloc[1]\n",
    "print(f\"   Balance ratio: {weapon_ratio:.2f}:1\")\n",
    "print(f\"   Challenge level: HIGH (Highly imbalanced)\")\n",
    "\n",
    "print(f\"\\nüìä RECOMMENDATION: Start with Crime Type Classification\")\n",
    "print(f\"   ‚úÖ Most complex and academically interesting\")\n",
    "print(f\"   ‚úÖ Real-world practical importance\")\n",
    "print(f\"   ‚úÖ Rich feature engineering opportunities\")\n",
    "print(f\"   ‚úÖ Multiple evaluation metrics possible\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c448a07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete ML Pipeline Design\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"COMPLETE ML PIPELINE DESIGN\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"üîß PREPROCESSING PIPELINE:\")\n",
    "print(\"-\" * 30)\n",
    "preprocessing_steps = [\n",
    "    \"1. Handle missing values (imputation strategies)\",\n",
    "    \"2. Temporal feature extraction (hour, day, month, season)\",\n",
    "    \"3. Categorical encoding (One-hot, Label encoding)\",\n",
    "    \"4. Geographical feature engineering (area clustering)\",\n",
    "    \"5. Numerical feature scaling (StandardScaler)\",\n",
    "    \"6. Feature selection (importance-based)\",\n",
    "    \"7. Handle class imbalance (SMOTE, class weights)\"\n",
    "]\n",
    "\n",
    "for step in preprocessing_steps:\n",
    "    print(f\"   {step}\")\n",
    "\n",
    "print(f\"\\nü§ñ MACHINE LEARNING MODELS TO COMPARE:\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "models_info = {\n",
    "    \"Random Forest\": {\n",
    "        \"type\": \"Ensemble\", \n",
    "        \"pros\": \"Handles missing values, feature importance, non-linear\",\n",
    "        \"cons\": \"Can overfit with many features\"\n",
    "    },\n",
    "    \"XGBoost\": {\n",
    "        \"type\": \"Gradient Boosting\",\n",
    "        \"pros\": \"High performance, handles imbalance, feature importance\",\n",
    "        \"cons\": \"Requires hyperparameter tuning\"\n",
    "    },\n",
    "    \"Logistic Regression\": {\n",
    "        \"type\": \"Linear\",\n",
    "        \"pros\": \"Fast, interpretable, good baseline\",\n",
    "        \"cons\": \"Assumes linear relationships\"\n",
    "    },\n",
    "    \"Support Vector Machine\": {\n",
    "        \"type\": \"Kernel-based\",\n",
    "        \"pros\": \"Good for high-dimensional data, non-linear kernels\",\n",
    "        \"cons\": \"Slow on large datasets\"\n",
    "    },\n",
    "    \"Neural Network\": {\n",
    "        \"type\": \"Deep Learning\",\n",
    "        \"pros\": \"Can capture complex patterns, flexible\",\n",
    "        \"cons\": \"Requires more data, black box\"\n",
    "    },\n",
    "    \"Naive Bayes\": {\n",
    "        \"type\": \"Probabilistic\",\n",
    "        \"pros\": \"Fast, works well with categorical data\",\n",
    "        \"cons\": \"Assumes feature independence\"\n",
    "    }\n",
    "}\n",
    "\n",
    "for i, (model, info) in enumerate(models_info.items(), 1):\n",
    "    print(f\"{i}. {model} ({info['type']})\")\n",
    "    print(f\"   ‚úÖ Pros: {info['pros']}\")\n",
    "    print(f\"   ‚ùå Cons: {info['cons']}\")\n",
    "    print()\n",
    "\n",
    "print(\"üìä EVALUATION STRATEGY:\")\n",
    "print(\"-\" * 25)\n",
    "evaluation_metrics = [\n",
    "    \"Accuracy (overall correctness)\",\n",
    "    \"F1-Score (macro & weighted for imbalanced classes)\",\n",
    "    \"Precision & Recall (per class)\",\n",
    "    \"Confusion Matrix (error analysis)\",\n",
    "    \"Classification Report (detailed per-class metrics)\",\n",
    "    \"ROC-AUC (if converted to binary problems)\",\n",
    "    \"Training Time (computational efficiency)\",\n",
    "    \"Cross-validation (5-fold stratified)\"\n",
    "]\n",
    "\n",
    "for metric in evaluation_metrics:\n",
    "    print(f\"   ‚Ä¢ {metric}\")\n",
    "\n",
    "print(f\"\\nüéØ RESEARCH CONTRIBUTION:\")\n",
    "print(\"-\" * 25)\n",
    "contributions = [\n",
    "    \"Compare multiple ML approaches on crime prediction\",\n",
    "    \"Analyze feature importance for crime type prediction\",\n",
    "    \"Handle real-world data challenges (missing values, imbalance)\",\n",
    "    \"Evaluate temporal and spatial pattern significance\",\n",
    "    \"Provide actionable insights for law enforcement\"\n",
    "]\n",
    "\n",
    "for contrib in contributions:\n",
    "    print(f\"   ‚Ä¢ {contrib}\")\n",
    "\n",
    "print(f\"\\nüìö BASELINE STUDIES TO COMPARE:\")\n",
    "print(\"-\" * 35)\n",
    "baselines = [\n",
    "    \"Crime prediction using machine learning (recent papers)\",\n",
    "    \"Temporal crime pattern analysis studies\",\n",
    "    \"Spatial crime hotspot prediction research\",\n",
    "    \"Ensemble methods for crime classification\",\n",
    "    \"Imbalanced learning techniques in criminology\"\n",
    "]\n",
    "\n",
    "for baseline in baselines:\n",
    "    print(f\"   ‚Ä¢ {baseline}\")\n",
    "\n",
    "print(f\"\\nüéì ACADEMIC PAPER STRUCTURE:\")\n",
    "print(\"-\" * 30)\n",
    "paper_sections = [\n",
    "    \"1. Introduction (ÿ£ŸáŸÖŸäÿ© ÿßŸÑÿ™ŸÜÿ®ÿ§ ÿ®ÿßŸÑÿ¨ÿ±ÿßÿ¶ŸÖ)\",\n",
    "    \"2. Literature Review (ÿßŸÑÿØÿ±ÿßÿ≥ÿßÿ™ ÿßŸÑÿ≥ÿßÿ®ŸÇÿ©)\", \n",
    "    \"3. Dataset Description (ŸàÿµŸÅ ÿßŸÑÿ®ŸäÿßŸÜÿßÿ™)\",\n",
    "    \"4. Methodology (ÿßŸÑŸÖŸÜŸáÿ¨Ÿäÿ© ŸàÿßŸÑŸÜŸÖÿßÿ∞ÿ¨)\",\n",
    "    \"5. Experiments & Results (ÿßŸÑÿ™ÿ¨ÿßÿ±ÿ® ŸàÿßŸÑŸÜÿ™ÿßÿ¶ÿ¨)\",\n",
    "    \"6. Discussion & Analysis (ÿßŸÑÿ™ÿ≠ŸÑŸäŸÑ ŸàÿßŸÑŸÖŸÜÿßŸÇÿ¥ÿ©)\",\n",
    "    \"7. Conclusion (ÿßŸÑÿÆŸÑÿßÿµÿ© ŸàÿßŸÑÿ™ŸàÿµŸäÿßÿ™)\"\n",
    "]\n",
    "\n",
    "for section in paper_sections:\n",
    "    print(f\"   {section}\")\n",
    "\n",
    "print(f\"\\n‚úÖ PROJECT FEASIBILITY: EXCELLENT\")\n",
    "print(f\"   üìä Rich dataset with real-world complexity\")\n",
    "print(f\"   üéØ Clear problem definition and practical importance\")\n",
    "print(f\"   üîß Multiple technical challenges to address\")\n",
    "print(f\"   üìà Strong potential for meaningful results\")\n",
    "print(f\"   üìö Sufficient literature for comparison\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## üéØ Final Recommendation: Crime Type Classification\n",
    "\n",
    "### **Problem Statement**\n",
    "**Predict crime type (`Crm Cd Desc`) based on temporal, spatial, and contextual features**\n",
    "\n",
    "### **Why This is the PERFECT Choice:**\n",
    "- ‚úÖ **Meets all project requirements** - complex, real-world, non-trivial\n",
    "- ‚úÖ **140 unique crime types** - challenging multi-class classification\n",
    "- ‚úÖ **Rich feature engineering opportunities** - temporal, spatial, demographic\n",
    "- ‚úÖ **High practical importance** - law enforcement resource allocation\n",
    "- ‚úÖ **Clear research contribution** - comparing multiple ML approaches\n",
    "- ‚úÖ **Sufficient complexity** - missing values, class imbalance, categorical data\n",
    "- ‚úÖ **Academic paper potential** - multiple technical challenges to discuss\n",
    "\n",
    "### **Success Metrics:**\n",
    "- **Primary**: Weighted F1-Score (handles class imbalance)\n",
    "- **Secondary**: Accuracy, Per-class Precision/Recall\n",
    "- **Analysis**: Confusion Matrix, Feature Importance, Error Analysis\n",
    "\n",
    "### **Expected Outcomes:**\n",
    "- **Technical**: 6 different ML models compared with hyperparameter tuning\n",
    "- **Academic**: 6-8 page research paper in Arabic\n",
    "- **Practical**: Actionable insights for crime prediction and prevention\n",
    "\n",
    "### **Next Steps:**\n",
    "1. **Run this notebook** to generate the cleaned dataset\n",
    "2. **Start with data preprocessing** pipeline\n",
    "3. **Focus on Crime Type Classification** as the primary problem\n",
    "4. **Implement baseline models first** (Logistic Regression, Random Forest)\n",
    "5. **Progress to advanced models** (XGBoost, Neural Networks)\n",
    "6. **Conduct thorough evaluation** and comparison\n",
    "\n",
    "---\n",
    "\n",
    "**üéì This project perfectly balances academic rigor with practical importance, ensuring you meet all requirements while working on a genuinely interesting and challenging problem!**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
